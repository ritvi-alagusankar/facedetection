{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import euclidean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amisha: 30 original images retained\n",
      "amisha: augmented to 35 images\n",
      "dhanoosh: 28 original images retained\n",
      "dhanoosh: augmented to 35 images\n",
      "jose: 35 original images retained\n",
      "jose: augmented to 35 images\n",
      "jui: 29 original images retained\n",
      "jui: augmented to 35 images\n",
      "ritvi: 36 original images retained\n",
      "ritvi: augmented to 35 images\n",
      "siddhangana: 26 original images retained\n",
      "siddhangana: augmented to 35 images\n",
      "sparsh: 36 original images retained\n",
      "sparsh: augmented to 35 images\n",
      "yash: 22 original images retained\n",
      "yash: augmented to 35 images\n",
      "Press 'q' to exit.\n"
     ]
    }
   ],
   "source": [
    "# === Load Face Detector ===\n",
    "hog_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def augment_image(image):\n",
    "    augmented = []\n",
    "    rows, cols = image.shape\n",
    "\n",
    "    for angle in [-5, 5]:\n",
    "        M = cv2.getRotationMatrix2D((cols // 2, rows // 2), angle, 1)\n",
    "        rotated = cv2.warpAffine(image, M, (cols, rows), borderMode=cv2.BORDER_REFLECT)\n",
    "        augmented.append(rotated)\n",
    "\n",
    "    for alpha, beta in [(1.1, 5), (0.9, -5)]:\n",
    "        adjusted = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "        augmented.append(adjusted)\n",
    "\n",
    "    return augmented\n",
    "\n",
    "def preprocess_face(img):\n",
    "    gray = cv2.equalizeHist(img)\n",
    "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(blurred)\n",
    "\n",
    "def load_training_data(png_dataset, target_images_per_person=35):\n",
    "    label_dict = {}\n",
    "    current_label = 0\n",
    "    person_images = {}\n",
    "\n",
    "    for person in os.listdir(png_dataset):\n",
    "        person_path = os.path.join(png_dataset, person)\n",
    "        if not os.path.isdir(person_path):\n",
    "            continue\n",
    "        label_dict[current_label] = person\n",
    "        valid_images = []\n",
    "\n",
    "        for img_name in os.listdir(person_path):\n",
    "            img_path = os.path.join(person_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            img = preprocess_face(img)\n",
    "            faces = hog_detector(img)\n",
    "            if len(faces) != 1:\n",
    "                continue\n",
    "\n",
    "            face = faces[0]\n",
    "            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "            x = max(0, x)\n",
    "            y = max(0, y)\n",
    "            x2 = min(img.shape[1], x + w)\n",
    "            y2 = min(img.shape[0], y + h)\n",
    "            face_img = img[y:y2, x:x2]\n",
    "\n",
    "            if face_img.size == 0 or face_img.shape[0] < 50 or face_img.shape[1] < 50:\n",
    "                continue\n",
    "\n",
    "            face_resized = cv2.resize(face_img, (200, 200))\n",
    "            valid_images.append(face_resized)\n",
    "\n",
    "        print(f\"{person}: {len(valid_images)} original images retained\")\n",
    "\n",
    "        augmented_images = valid_images.copy()\n",
    "        i = 0\n",
    "        while len(augmented_images) < target_images_per_person:\n",
    "            augmented = augment_image(valid_images[i % len(valid_images)])\n",
    "            for img in augmented:\n",
    "                if len(augmented_images) >= target_images_per_person:\n",
    "                    break\n",
    "                augmented_images.append(img)\n",
    "            i += 1\n",
    "\n",
    "        person_images[current_label] = augmented_images[:target_images_per_person]\n",
    "        print(f\"{person}: augmented to {len(person_images[current_label])} images\")\n",
    "        current_label += 1\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for label, img_list in person_images.items():\n",
    "        images.extend(img_list)\n",
    "        labels.extend([label] * len(img_list))\n",
    "\n",
    "    return images, np.array(labels), label_dict\n",
    "\n",
    "class DCVAHybridClassifier:\n",
    "    def __init__(self, eigenfaces, mean_vector):\n",
    "        self.eigenfaces = eigenfaces\n",
    "        self.mean = mean_vector\n",
    "        self.class_vectors = {}\n",
    "        self.labels = []\n",
    "        self.label_names = {}\n",
    "\n",
    "    def fit(self, faces, labels, label_names):\n",
    "        self.label_names = label_names\n",
    "        features = []\n",
    "        for img in faces:\n",
    "            flat = img.flatten().astype(np.float32).reshape(1, -1)\n",
    "            proj = (flat - self.mean) @ self.eigenfaces\n",
    "            features.append(proj.flatten())\n",
    "\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        class_features = defaultdict(list)\n",
    "        for feat, lbl in zip(features, labels):\n",
    "            class_features[lbl].append(feat)\n",
    "\n",
    "        for lbl, feats in class_features.items():\n",
    "            feats = np.array(feats)\n",
    "            mean_vector = np.mean(feats, axis=0)\n",
    "            residuals = feats - mean_vector\n",
    "            U, S, Vt = np.linalg.svd(residuals, full_matrices=False)\n",
    "            null_space = Vt[S < 1e-5]\n",
    "            if null_space.size == 0:\n",
    "                common_vec = mean_vector\n",
    "            else:\n",
    "                proj = residuals @ null_space.T @ null_space\n",
    "                common_vec = mean_vector + proj.mean(axis=0)\n",
    "            self.class_vectors[lbl] = normalize(common_vec.reshape(1, -1))[0]\n",
    "\n",
    "    def predict(self, img, method=\"cosine\"):\n",
    "        flat = img.flatten().astype(np.float32).reshape(1, -1)\n",
    "        proj = (flat - self.mean) @ self.eigenfaces\n",
    "        test_vec = normalize(proj)[0]\n",
    "\n",
    "        best_label = None\n",
    "        best_score = float(\"inf\") if method == \"euclidean\" else -1\n",
    "\n",
    "        for lbl, class_vec in self.class_vectors.items():\n",
    "            if method == \"cosine\":\n",
    "                score = np.dot(test_vec, class_vec)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_label = lbl\n",
    "            else:\n",
    "                score = euclidean(test_vec, class_vec)\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_label = lbl\n",
    "\n",
    "        if best_label is None:\n",
    "            return None, 0.0\n",
    "        else:\n",
    "            return best_label, np.clip(best_score, 0.0, 1.0)\n",
    "\n",
    "faces, labels, label_dict = load_training_data(\"png_dataset/png_dataset\")\n",
    "recognizer = cv2.face.FisherFaceRecognizer_create()\n",
    "recognizer.train(faces, labels)\n",
    "eigenfaces = recognizer.getEigenVectors()\n",
    "mean = recognizer.getMean().reshape(1, -1)\n",
    "\n",
    "dcva = DCVAHybridClassifier(eigenfaces, mean)\n",
    "dcva.fit(faces, labels, label_dict)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press 'q' to exit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    processed = preprocess_face(gray)\n",
    "    detected_faces = hog_detector(processed)\n",
    "\n",
    "    for face in detected_faces:\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        x2, y2 = min(gray.shape[1], x + w), min(gray.shape[0], y + h)\n",
    "\n",
    "        face_img = processed[y:y2, x:x2]\n",
    "        if face_img.size == 0:\n",
    "            continue\n",
    "\n",
    "        face_resized = cv2.resize(face_img, (200, 200))\n",
    "        label, score = dcva.predict(face_resized, method=\"cosine\")\n",
    "        threshold = 0.5\n",
    "\n",
    "        if label is not None and score > threshold:\n",
    "            name = label_dict.get(label, \"Unknown\")\n",
    "        else:\n",
    "            name = \"Unknown\"\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{name} ({score:.2f})\", (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Fisherface + DCVA Recognition (Press 'q' to quit)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
