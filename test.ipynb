{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize the webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Define the aspect ratio for a typical face\n",
    "# EXPECTED_ASPECT_RATIO = 1.6  # Height/Width ratio\n",
    "\n",
    "# while True:\n",
    "#     # Capture frame-by-frame\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Convert to grayscale\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Apply Gaussian blur to reduce noise\n",
    "#     blurred = cv2.GaussianBlur(gray, (5, 5), 1.6)\n",
    "\n",
    "#     # Perform Canny edge detection\n",
    "#     edges = cv2.Canny(blurred, threshold1=100, threshold2=200)\n",
    "\n",
    "#     # Find contours in the edge-detected image\n",
    "#     contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     # Iterate through each contour\n",
    "#     for contour in contours:\n",
    "#         # Fit an ellipse to the contour if it has enough points\n",
    "#         if len(contour) >= 5:\n",
    "#             ellipse = cv2.fitEllipse(contour)\n",
    "#             center, axes, angle = ellipse\n",
    "#             major_axis, minor_axis = max(axes), min(axes)\n",
    "#             aspect_ratio = major_axis / (minor_axis + 0.00000001)\n",
    "\n",
    "#             # Check if the aspect ratio is within a reasonable range\n",
    "#             if 1.4 < aspect_ratio < 1.8:\n",
    "#                 # Draw the ellipse on the original frame\n",
    "#                 cv2.ellipse(frame, ellipse, (0, 255, 0), 2)\n",
    "\n",
    "#                 # Optionally, draw the center and axes\n",
    "#                 cv2.circle(frame, (int(center[0]), int(center[1])), 5, (0, 0, 255), -1)\n",
    "#                 cv2.line(frame, (int(center[0] - axes[0] / 2), int(center[1])), \n",
    "#                          (int(center[0] + axes[0] / 2), int(center[1])), (255, 0, 0), 2)\n",
    "#                 cv2.line(frame, (int(center[0]), int(center[1] - axes[1] / 2)), \n",
    "#                          (int(center[0]), int(center[1] + axes[1] / 2)), (255, 0, 0), 2)\n",
    "\n",
    "#     # Display the resulting frame\n",
    "#     cv2.imshow('Real-Time Face Detection', frame)\n",
    "\n",
    "#     # Break the loop on 'q' key press\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the webcam and close all OpenCV windows\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/face_dataset/merged/labels/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m hog_detector \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Pre-load label filenames into a dictionary for fast lookup\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m label_files \u001b[38;5;241m=\u001b[39m {os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(f)[\u001b[38;5;241m0\u001b[39m]: f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(labels_path)}\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Iterate over all image files\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dataset_path):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/face_dataset/merged/labels/train'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset paths\n",
    "dataset_path = \"/content/face_dataset/merged/images/train\"\n",
    "labels_path = \"/content/face_dataset/merged/labels/train\"\n",
    "\n",
    "# Process a smaller subset first\n",
    "MAX_IMAGES = 100  # Change as needed\n",
    "image_count = 0  # Counter for processed images\n",
    "\n",
    "face_size = (64, 64)\n",
    "faces = []\n",
    "labels = []\n",
    "detected_images_list = []  # Store images with bounding boxes for visualization\n",
    "missing_labels = 0\n",
    "\n",
    "# Load HOG + SVM-based face detector from dlib\n",
    "hog_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Pre-load label filenames into a dictionary for fast lookup\n",
    "label_files = {os.path.splitext(f)[0]: f for f in os.listdir(labels_path)}\n",
    "\n",
    "# Iterate over all image files\n",
    "for image_name in os.listdir(dataset_path):\n",
    "    if image_count >= MAX_IMAGES:\n",
    "        break  # Stop processing after reaching MAX_IMAGES\n",
    "    image_count += 1\n",
    "\n",
    "    image_path = os.path.join(dataset_path, image_name)\n",
    "\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Skipping unreadable image: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.equalizeHist(gray)  # Improves contrast\n",
    "\n",
    "    # Detect faces using HOG + SVM\n",
    "    detected_faces = hog_detector(gray)\n",
    "\n",
    "    if len(detected_faces) == 0:\n",
    "        continue  # Skip images with no faces\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for face in detected_faces:\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box around detected face\n",
    "\n",
    "    # Store the image with bounding boxes\n",
    "    detected_images_list.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert for Matplotlib display\n",
    "\n",
    "    # Match labels faster\n",
    "    base_name = os.path.splitext(image_name)[0]  # Remove file extension\n",
    "    label_filename = label_files.get(base_name, None)  # Find label file\n",
    "\n",
    "    if label_filename:\n",
    "        label_path = os.path.join(labels_path, label_filename)\n",
    "        with open(label_path, \"r\") as label_file:\n",
    "            label = label_file.read().strip()\n",
    "            labels.append(label)\n",
    "    else:\n",
    "        missing_labels += 1\n",
    "        labels.append(\"Unknown\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "faces = np.array(faces)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Total faces detected: {len(detected_images_list)}\")\n",
    "print(f\"Total missing labels: {missing_labels}\")  # Print at the end\n",
    "\n",
    "# ðŸ“Œ Display multiple images in 2-3 rows with bounding boxes\n",
    "num_images_to_display = min(9, len(detected_images_list))  # Display up to 9 images\n",
    "rows = 3\n",
    "cols = num_images_to_display // rows if num_images_to_display % rows == 0 else (num_images_to_display // rows) + 1\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_images_to_display):\n",
    "    axes[i].imshow(detected_images_list[i])  # Show image with bounding box\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Image {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34364\\924397068.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Capture frame-by-frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the aspect ratio for a typical face\n",
    "EXPECTED_ASPECT_RATIO = 1.6  # Height/Width ratio\n",
    "\n",
    "# Initialize OpenCV's Kalman Filter for tracking\n",
    "kalman = cv2.KalmanFilter(4, 2)  # 4 state variables (x, y, dx, dy) and 2 measurement variables (x, y)\n",
    "kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], dtype=np.float32)\n",
    "kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], dtype=np.float32)\n",
    "kalman.processNoiseCov = np.array([[1e-2, 0, 0, 0], [0, 1e-2, 0, 0], [0, 0, 1e-5, 0], [0, 0, 0, 1e-5]], dtype=np.float32)\n",
    "kalman.measurementNoiseCov = np.array([[1, 0], [0, 1]], dtype=np.float32)\n",
    "\n",
    "# Previous frame for motion detection\n",
    "prev_gray = None\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert to YCbCr color space for skin segmentation\n",
    "    ycbcr = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "    # Define skin color range\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "\n",
    "    # Skin mask using the defined range\n",
    "    skin_mask = cv2.inRange(ycbcr, lower_skin, upper_skin)\n",
    "\n",
    "    # Apply the mask to the frame\n",
    "    skin = cv2.bitwise_and(frame, frame, mask=skin_mask)\n",
    "\n",
    "    # Perform frame differencing for motion detection (if previous frame exists)\n",
    "    if prev_gray is not None:\n",
    "        frame_diff = cv2.absdiff(prev_gray, gray)\n",
    "        _, thresh = cv2.threshold(frame_diff, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours in the thresholded difference image (motion regions)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            if len(contour) >= 5:\n",
    "                ellipse = cv2.fitEllipse(contour)\n",
    "                center, axes, angle = ellipse\n",
    "                major_axis, minor_axis = max(axes), min(axes)\n",
    "                aspect_ratio = major_axis / (minor_axis + 0.00000001)\n",
    "                # Eccentricity of the ellipse for face validation\n",
    "                eccentricity = np.sqrt(1 - (minor_axis / major_axis) ** 2)\n",
    "\n",
    "                # Check if the contour fits within the expected face shape\n",
    "                if 1.4 < aspect_ratio < 1.8 and eccentricity < 0.9:\n",
    "                    cv2.ellipse(frame, ellipse, (0, 255, 0), 2)\n",
    "                    # Kalman filter tracking for predicted face position\n",
    "                    measurement = np.array([[np.float32(center[0])], [np.float32(center[1])]])\n",
    "                    kalman.correct(measurement)\n",
    "                    prediction = kalman.predict()\n",
    "\n",
    "                    predicted_center = (int(prediction[0]), int(prediction[1]))\n",
    "                    cv2.circle(frame, predicted_center, 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Store the current grayscale frame for the next iteration\n",
    "    prev_gray = gray\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Real-Time Face Detection', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize OpenCV's HOG + SVM face detector\n",
    "hog_face_detector = cv2.HOGDescriptor()\n",
    "hog_face_detector.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Initialize OpenCV's Kalman Filter for tracking\n",
    "kalman = cv2.KalmanFilter(4, 2)  # 4 state variables (x, y, dx, dy) and 2 measurement variables (x, y)\n",
    "kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], dtype=np.float32)\n",
    "kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], dtype=np.float32)\n",
    "kalman.processNoiseCov = np.array([[1e-2, 0, 0, 0], [0, 1e-2, 0, 0], [0, 0, 1e-5, 0], [0, 0, 0, 1e-5]], dtype=np.float32)\n",
    "kalman.measurementNoiseCov = np.array([[1, 0], [0, 1]], dtype=np.float32)\n",
    "\n",
    "# Previous frame for motion detection\n",
    "prev_gray = None\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # HOG face detection (alternative method)\n",
    "    faces, _ = hog_face_detector.detectMultiScale(gray, winStride=(8, 8), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # Draw bounding boxes for faces detected using HOG + SVM\n",
    "    for (x, y, w, h) in faces:\n",
    "        print(x,y,w,h)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Optionally, apply Kalman Filter for tracking detected faces (only for motion-based tracking)\n",
    "    if prev_gray is not None:\n",
    "        frame_diff = cv2.absdiff(prev_gray, gray)\n",
    "        _, thresh = cv2.threshold(frame_diff, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours in the thresholded difference image (motion regions)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            if len(contour) >= 5:\n",
    "                ellipse = cv2.fitEllipse(contour)\n",
    "                center, axes, angle = ellipse\n",
    "                major_axis, minor_axis = max(axes), min(axes)\n",
    "                aspect_ratio = major_axis / (minor_axis + 0.00000000001)\n",
    "                # Eccentricity of the ellipse for face validation\n",
    "                eccentricity = np.sqrt(1 - (minor_axis / major_axis) ** 2)\n",
    "\n",
    "                # Check if the contour fits within the expected face shape\n",
    "                if 1.4 < aspect_ratio < 1.8 and eccentricity < 0.9:\n",
    "                    cv2.ellipse(frame, ellipse, (0, 255, 0), 2)\n",
    "                    # Kalman filter tracking for predicted face position\n",
    "                    measurement = np.array([[np.float32(center[0])], [np.float32(center[1])]])\n",
    "                    kalman.correct(measurement)\n",
    "                    prediction = kalman.predict()\n",
    "\n",
    "                    predicted_center = (int(prediction[0]), int(prediction[1]))\n",
    "                    cv2.circle(frame, predicted_center, 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Store the current grayscale frame for the next iteration\n",
    "    prev_gray = gray\n",
    "\n",
    "    # Display the resulting frame with bounding boxes\n",
    "    cv2.imshow('Real-Time Face Detection', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LBPH model found. Only detection will work.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize OpenCV's HOG + SVM face detector\n",
    "hog_face_detector = cv2.HOGDescriptor()\n",
    "hog_face_detector.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Initialize Local Binary Pattern Histogram (LBPH) Face Recognizer\n",
    "lbph_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "# Load pre-trained LBPH model (if available)\n",
    "try:\n",
    "    lbph_recognizer.read(\"lbph_face_model.xml\")  # Ensure you have a trained model\n",
    "    print(\"LBPH model loaded successfully.\")\n",
    "except:\n",
    "    print(\"No LBPH model found. Only detection will work.\")\n",
    "\n",
    "# Initialize Kalman Filter for face tracking\n",
    "kalman = cv2.KalmanFilter(4, 2)  # 4 state variables (x, y, dx, dy), 2 measurements (x, y)\n",
    "kalman.transitionMatrix = np.array([[1, 0, 1, 0], \n",
    "                                    [0, 1, 0, 1], \n",
    "                                    [0, 0, 1, 0], \n",
    "                                    [0, 0, 0, 1]], dtype=np.float32)\n",
    "kalman.measurementMatrix = np.array([[1, 0, 0, 0], \n",
    "                                     [0, 1, 0, 0]], dtype=np.float32)\n",
    "kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 1e-2\n",
    "kalman.measurementNoiseCov = np.eye(2, dtype=np.float32) * 1\n",
    "\n",
    "# Variables for tracking\n",
    "tracking = False\n",
    "tracked_bbox = None\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # HOG-SVM Face Detection\n",
    "    faces, _ = hog_face_detector.detectMultiScale(gray, winStride=(8, 8), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # If a face is detected, update the Kalman filter\n",
    "    if len(faces) > 0:\n",
    "        tracking = True  # Start tracking when a face is detected\n",
    "        x, y, w, h = faces[0]  # Take the first detected face\n",
    "\n",
    "        # Update Kalman Filter with the detected bounding box center\n",
    "        center_x, center_y = x + w // 2, y + h // 2\n",
    "        measurement = np.array([[np.float32(center_x)], [np.float32(center_y)]])\n",
    "        kalman.correct(measurement)\n",
    "\n",
    "        # Save detected face coordinates\n",
    "        tracked_bbox = (x, y, w, h)\n",
    "\n",
    "        # Recognize face with LBPH (if trained)\n",
    "        face_roi = gray[y:y+h, x:x+w]\n",
    "        try:\n",
    "            label, confidence = lbph_recognizer.predict(face_roi)\n",
    "            text = f\"ID: {label} | Conf: {round(confidence, 2)}\"\n",
    "            cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        except:\n",
    "            pass  # If no trained model, skip recognition\n",
    "\n",
    "        # Draw bounding box around the detected face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    if tracking and tracked_bbox:\n",
    "        # Predict next position of the face\n",
    "        prediction = kalman.predict()\n",
    "        predicted_x, predicted_y = int(prediction[0]), int(prediction[1])\n",
    "\n",
    "        # Draw the predicted position\n",
    "        cv2.circle(frame, (predicted_x, predicted_y), 5, (0, 0, 255), -1)\n",
    "        cv2.putText(frame, \"Tracking\", (predicted_x, predicted_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Real-Time Face Detection & Tracking', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LBPH model found. Only detection will work.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31416\\44688017.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# Compute bounding box from the tracked points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_new\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mx_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0my_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mx_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize OpenCV's HOG + SVM face detector\n",
    "hog_face_detector = cv2.HOGDescriptor()\n",
    "hog_face_detector.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Initialize Local Binary Pattern Histogram (LBPH) Face Recognizer\n",
    "lbph_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "# Load pre-trained LBPH model (if available)\n",
    "try:\n",
    "    lbph_recognizer.read(\"lbph_face_model.xml\")  # Ensure you have a trained model\n",
    "    print(\"LBPH model loaded successfully.\")\n",
    "except:\n",
    "    print(\"No LBPH model found. Only detection will work.\")\n",
    "\n",
    "# Variables for tracking\n",
    "tracking = False\n",
    "tracked_points = None\n",
    "old_gray = None\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # HOG-SVM Face Detection\n",
    "    faces, _ = hog_face_detector.detectMultiScale(gray, winStride=(8, 8), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # If a face is detected, start tracking or reinitialize tracking\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]  # Take the first detected face\n",
    "\n",
    "        # Initialize tracking points (corners of the bounding box)\n",
    "        tracked_points = np.array([[x, y], [x + w, y], [x, y + h], [x + w, y + h]], dtype=np.float32).reshape(-1, 1, 2)\n",
    "        old_gray = gray.copy()  # Save current frame for optical flow\n",
    "        tracking = True\n",
    "\n",
    "        # Recognize face with LBPH (if trained)\n",
    "        face_roi = gray[y:y+h, x:x+w]\n",
    "        try:\n",
    "            label, confidence = lbph_recognizer.predict(face_roi)\n",
    "            text = f\"ID: {label} | Conf: {round(confidence, 2)}\"\n",
    "            cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        except:\n",
    "            pass  # If no trained model, skip recognition\n",
    "\n",
    "        # Draw bounding box around the detected face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    elif tracking and tracked_points is not None:\n",
    "        # Calculate optical flow (i.e., track points)\n",
    "        next_points, status, _ = cv2.calcOpticalFlowPyrLK(old_gray, gray, tracked_points, None, **lk_params)\n",
    "\n",
    "        # Filter out bad points\n",
    "        good_new = next_points[status == 1]\n",
    "        good_old = tracked_points[status == 1]\n",
    "\n",
    "        # Compute bounding box from the tracked points\n",
    "        if len(good_new) > 0:\n",
    "            x_min = int(np.min(good_new[:, 0, 0]))\n",
    "            y_min = int(np.min(good_new[:, 0, 1]))\n",
    "            x_max = int(np.max(good_new[:, 0, 0]))\n",
    "            y_max = int(np.max(good_new[:, 0, 1]))\n",
    "            \n",
    "            # Update the bounding box with the new coordinates\n",
    "            tracked_bbox = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "            # Draw the new bounding box\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "            # Update the previous frame and previous points\n",
    "            old_gray = gray.copy()\n",
    "            tracked_points = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "        else:\n",
    "            # If tracking failed, stop tracking\n",
    "            tracking = False\n",
    "            tracked_points = None\n",
    "            print(\"Lost track of face.\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Real-Time Face Detection & Tracking', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preparing training data from: dataset/dataset\n",
      "[INFO] Processing images for: amisha\n",
      "[WARNING] No face detected in: dataset/dataset\\amisha\\ezgif-frame-030.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\amisha\\ezgif-frame-040.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\amisha\\IMG-20250331-WA0023.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\amisha\\IMG-20250331-WA0028.jpg. Skipping.\n",
      "[INFO] Processing images for: dhanoosh\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-060.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-130.jpg. Skipping.\n",
      "[INFO] Processing images for: jose\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\ezgif-frame-005.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\ezgif-frame-015.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\ezgif-frame-035.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\IMG-20250331-WA0039.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\photo_1_2025-03-13_14-23-01.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\photo_3_2025-03-13_14-23-01.jpg. Skipping.\n",
      "[INFO] Processing images for: jui\n",
      "[WARNING] Could not read image: dataset/dataset\\jui\\IMG_5901.HEIC. Skipping.\n",
      "[INFO] Processing images for: ritvi\n",
      "[WARNING] No face detected in: dataset/dataset\\ritvi\\ezgif-frame-015.jpg. Skipping.\n",
      "[INFO] Processing images for: siddhangana\n",
      "[WARNING] No face detected in: dataset/dataset\\siddhangana\\photo_3_2025-03-13_14-23-34.jpg. Skipping.\n",
      "[INFO] Processing images for: sparsh\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-005.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-010.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-015.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-020.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-025.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-030.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-035.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\photo_4_2025-03-06_12-21-48.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\photo_6_2025-03-06_12-21-48.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\photo_8_2025-03-06_12-21-48.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\photo_9_2025-03-06_12-21-48.jpg. Skipping.\n",
      "[INFO] Processing images for: yash\n",
      "[WARNING] No face detected in: dataset/dataset\\yash\\photo_12_2025-03-06_12-21-48.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\yash\\photo_15_2025-03-06_12-21-48.jpg. Skipping.\n",
      "[WARNING] Invalid face bounds detected in: dataset/dataset\\yash\\photo_18_2025-03-06_12-21-48.jpg. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\yash\\photo_5_2025-03-06_14-34-55.jpg. Skipping.\n",
      "[WARNING] Invalid face bounds detected in: dataset/dataset\\yash\\photo_7_2025-03-06_14-34-55.jpg. Skipping.\n",
      "[WARNING] Invalid face bounds detected in: dataset/dataset\\yash\\photo_9_2025-03-06_14-34-55.jpg. Skipping.\n",
      "[INFO] Total faces prepared for training: 146\n",
      "[INFO] Total unique persons (labels): 8\n",
      "[INFO] Training LBPH model...\n",
      "[INFO] LBPH model trained successfully.\n",
      "[INFO] Starting video stream...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time # For FPS calculation (optional)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Main dataset folder containing subfolders named after people\n",
    "# Example structure:\n",
    "# /path/to/your/dataset/\n",
    "#  |- Person_A/\n",
    "#  |  |- img1.jpg\n",
    "#  |  |- img2.png\n",
    "#  |- Person_B/\n",
    "#  |  |- pic1.jpeg\n",
    "# ...\n",
    "dataset_path = \"dataset/dataset\" \n",
    "\n",
    "# Size to resize cropped faces to (consistency is key for LBPH)\n",
    "face_size = (100, 100)\n",
    "\n",
    "# LBPH Confidence threshold (lower values mean higher confidence)\n",
    "# Adjust this based on testing - might need values between 50 and 100\n",
    "confidence_threshold = 25\n",
    "\n",
    "# --- Global Variables ---\n",
    "# Load Dlib's pre-trained frontal face detector (HOG-based)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Create LBPH Face Recognizer\n",
    "# Note: You might need to install opencv-contrib-python\n",
    "# pip install opencv-contrib-python\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "# Dictionary to map numerical labels to names\n",
    "label_to_name = {}\n",
    "\n",
    "# --- Function to Prepare Training Data and Train LBPH ---\n",
    "def train_model(data_folder_path):\n",
    "    \"\"\"\n",
    "    Scans the dataset folder, detects faces, prepares training data,\n",
    "    and trains the LBPH recognizer.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Preparing training data from: {data_folder_path}\")\n",
    "    faces = []\n",
    "    labels = []\n",
    "    current_label_id = 0\n",
    "    global label_to_name # Allow modification of the global dictionary\n",
    "\n",
    "    # Iterate through each person's folder in the dataset path\n",
    "    for person_name in os.listdir(data_folder_path):\n",
    "        person_folder_path = os.path.join(data_folder_path, person_name)\n",
    "\n",
    "        # Skip if it's not a directory\n",
    "        if not os.path.isdir(person_folder_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing images for: {person_name}\")\n",
    "        # Assign a numerical label to this person if not already done\n",
    "        if person_name not in label_to_name.values():\n",
    "            label_to_name[current_label_id] = person_name\n",
    "            person_label = current_label_id\n",
    "            current_label_id += 1\n",
    "        else:\n",
    "            # Find existing label ID if name already exists (shouldn't happen with unique folder names)\n",
    "            person_label = [id for id, name in label_to_name.items() if name == person_name][0]\n",
    "\n",
    "\n",
    "        # Iterate through images in the person's folder\n",
    "        for image_name in os.listdir(person_folder_path):\n",
    "            image_path = os.path.join(person_folder_path, image_name)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"[WARNING] Could not read image: {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Convert to grayscale (Dlib detector and LBPH work better with grayscale)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # Optional: Histogram Equalization can sometimes improve detection/recognition\n",
    "            # gray = cv2.equalizeHist(gray)\n",
    "\n",
    "            # Detect faces using Dlib detector\n",
    "            # The '1' indicates upsampling the image 1 time, making it larger\n",
    "            # and potentially detecting more faces, but it's slower. Use 0 for faster.\n",
    "            detected_faces = detector(gray, 1)\n",
    "\n",
    "            if len(detected_faces) == 0:\n",
    "                print(f\"[WARNING] No face detected in: {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Process only the first detected face for simplicity in training\n",
    "            face_rect = detected_faces[0]\n",
    "            x, y, w, h = face_rect.left(), face_rect.top(), face_rect.width(), face_rect.height()\n",
    "\n",
    "            # Ensure coordinates are valid\n",
    "            if x < 0 or y < 0 or w <= 0 or h <= 0:\n",
    "                print(f\"[WARNING] Invalid face bounds detected in: {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Crop the face region from the grayscale image\n",
    "            face_roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "            # Check if cropping resulted in an empty image\n",
    "            if face_roi.size == 0:\n",
    "                 print(f\"[WARNING] Empty face ROI after cropping in: {image_path}. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            # Resize the face ROI to the standard size\n",
    "            resized_face = cv2.resize(face_roi, face_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Append the processed face and its corresponding label\n",
    "            faces.append(resized_face)\n",
    "            labels.append(person_label)\n",
    "\n",
    "    if not faces:\n",
    "        print(\"[ERROR] No faces found for training. Check dataset path and image content.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"[INFO] Total faces prepared for training: {len(faces)}\")\n",
    "    print(f\"[INFO] Total unique persons (labels): {len(label_to_name)}\")\n",
    "    print(\"[INFO] Training LBPH model...\")\n",
    "\n",
    "    # Train the recognizer\n",
    "    # Ensure labels is a NumPy array of type int32\n",
    "    recognizer.train(faces, np.array(labels, dtype=np.int32))\n",
    "\n",
    "    print(\"[INFO] LBPH model trained successfully.\")\n",
    "    # Optional: Save the trained model and label mapping for later use\n",
    "    # recognizer.save(\"lbph_model.yml\")\n",
    "    # with open(\"label_map.txt\", \"w\") as f:\n",
    "    #    for label_id, name in label_to_name.items():\n",
    "    #        f.write(f\"{label_id}:{name}\\n\")\n",
    "    # print(\"[INFO] Model and label map saved.\")\n",
    "    return True\n",
    "\n",
    "# --- Function for Live Face Recognition ---\n",
    "def run_live_recognition():\n",
    "    \"\"\"\n",
    "    Opens the camera, detects faces, and performs recognition using the trained model.\n",
    "    \"\"\"\n",
    "    global label_to_name # Access the global mapping\n",
    "\n",
    "    print(\"[INFO] Starting video stream...\")\n",
    "    # Use 0 for default camera, or change if you have multiple cameras\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    time.sleep(1.0) # Allow camera sensor to warm up\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"[ERROR] Cannot open camera. Exiting.\")\n",
    "        return\n",
    "\n",
    "    prev_time = 0\n",
    "\n",
    "    while True:\n",
    "        # Read frame from camera\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            print(\"[ERROR] Failed to grab frame. Exiting.\")\n",
    "            break\n",
    "\n",
    "        # For FPS calculation\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "\n",
    "        # Convert frame to grayscale for detection\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Optional: Equalize histogram\n",
    "        # gray_frame = cv2.equalizeHist(gray_frame)\n",
    "\n",
    "        # Detect faces in the current frame\n",
    "        detected_faces = detector(gray_frame, 0) # Use 0 for speed in live feed\n",
    "\n",
    "        # Loop over detected faces\n",
    "        for face_rect in detected_faces:\n",
    "            x, y, w, h = face_rect.left(), face_rect.top(), face_rect.width(), face_rect.height()\n",
    "\n",
    "            # Ensure coordinates are valid and within frame boundaries\n",
    "            x = max(0, x)\n",
    "            y = max(0, y)\n",
    "            w = min(w, frame.shape[1] - x)\n",
    "            h = min(h, frame.shape[0] - y)\n",
    "\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue # Skip invalid detections\n",
    "\n",
    "            # Crop the face region *from the grayscale frame* for recognition\n",
    "            face_roi = gray_frame[y:y+h, x:x+w]\n",
    "\n",
    "             # Check if cropping resulted in an empty image\n",
    "            if face_roi.size == 0:\n",
    "                 continue\n",
    "\n",
    "            # Resize the cropped face to the standard size used for training\n",
    "            resized_face = cv2.resize(face_roi, face_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Perform prediction using the trained LBPH recognizer\n",
    "            label_id, confidence = recognizer.predict(resized_face)\n",
    "\n",
    "            # Get the name associated with the predicted label ID\n",
    "            # Use \"Unknown\" if confidence is too high (poor match) or label unknown\n",
    "            if confidence < confidence_threshold and label_id in label_to_name:\n",
    "                name = label_to_name[label_id]\n",
    "                display_text = f\"{name} ({confidence:.2f})\"\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "                display_text = f\"{name} ({confidence:.2f})\" # Still show confidence for unknowns\n",
    "\n",
    "            # Draw bounding box around the face on the *original color* frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Put the predicted name (and confidence) text above the bounding box\n",
    "            text_y = y - 10 if y - 10 > 10 else y + 10 # Position text above box\n",
    "            cv2.putText(frame, display_text, (x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Display FPS on frame (optional)\n",
    "        cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Show the resulting frame\n",
    "        cv2.imshow(\"Live Face Recognition (Press 'q' to quit)\", frame)\n",
    "\n",
    "        # Check for exit key ('q')\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"[INFO] Stopping video stream and cleaning up...\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"[INFO] Application finished.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.isdir(dataset_path):\n",
    "         print(f\"[ERROR] Dataset path not found or is not a directory: {dataset_path}\")\n",
    "         print(\"[INFO] Please set the 'dataset_path' variable correctly.\")\n",
    "    elif train_model(dataset_path): # Train the model first\n",
    "        run_live_recognition() # If training is successful, run live recognition\n",
    "    else:\n",
    "        print(\"[ERROR] Model training failed. Cannot proceed to live recognition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preparing training data from: dataset/dataset\n",
      "[INFO] Processing images for: amisha\n",
      "[WARNING] No face detected in: dataset/dataset\\amisha\\ezgif-frame-030.jpg for amisha. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\amisha\\ezgif-frame-040.jpg for amisha. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\amisha\\IMG-20250331-WA0023.jpg for amisha. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\amisha\\IMG-20250331-WA0028.jpg for amisha. Skipping.\n",
      "[INFO] Added 17 face images for amisha.\n",
      "[INFO] Processing images for: dhanoosh\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-058.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-059.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-060.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-061.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-062.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-063.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-064.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-065.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-066.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-090.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-091.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-092.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-093.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-094.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-095.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-096.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-097.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-098.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-099.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-100.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-101.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-102.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-103.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-104.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-105.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-106.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-124.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-125.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-126.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-127.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-128.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-129.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-130.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-131.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-132.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-133.png for dhanoosh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\dhanoosh\\ezgif-frame-135.png for dhanoosh. Skipping.\n",
      "[INFO] Added 64 face images for dhanoosh.\n",
      "[INFO] Processing images for: jose\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\ezgif-frame-005.jpg for jose. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\ezgif-frame-015.jpg for jose. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\ezgif-frame-035.jpg for jose. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\IMG-20250331-WA0039.jpg for jose. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\photo_1_2025-03-13_14-23-01.jpg for jose. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\jose\\photo_3_2025-03-13_14-23-01.jpg for jose. Skipping.\n",
      "[INFO] Added 23 face images for jose.\n",
      "[INFO] Processing images for: jui\n",
      "[WARNING] Could not read image: dataset/dataset\\jui\\IMG_5901.HEIC. Skipping.\n",
      "[WARNING] Multiple faces (3) detected in: dataset/dataset\\jui\\IMG_9940.PNG for jui. Using the first one.\n",
      "[WARNING] Multiple faces (3) detected in: dataset/dataset\\jui\\IMG_9941.PNG for jui. Using the first one.\n",
      "[WARNING] Multiple faces (3) detected in: dataset/dataset\\jui\\IMG_9942.PNG for jui. Using the first one.\n",
      "[WARNING] Multiple faces (3) detected in: dataset/dataset\\jui\\IMG_9943.PNG for jui. Using the first one.\n",
      "[WARNING] Multiple faces (3) detected in: dataset/dataset\\jui\\IMG_9944.PNG for jui. Using the first one.\n",
      "[WARNING] Multiple faces (3) detected in: dataset/dataset\\jui\\IMG_9945.PNG for jui. Using the first one.\n",
      "[WARNING] Multiple faces (3) detected in: dataset/dataset\\jui\\IMG_9946.PNG for jui. Using the first one.\n",
      "[WARNING] Multiple faces (3) detected in: dataset/dataset\\jui\\IMG_9947.PNG for jui. Using the first one.\n",
      "[WARNING] Invalid face bounds detected or calculated in: dataset/dataset\\jui\\IMG_9955.PNG. Skipping.\n",
      "[INFO] Added 18 face images for jui.\n",
      "[INFO] Processing images for: ritvi\n",
      "[WARNING] No face detected in: dataset/dataset\\ritvi\\ezgif-frame-015.jpg for ritvi. Skipping.\n",
      "[INFO] Added 31 face images for ritvi.\n",
      "[INFO] Processing images for: siddhangana\n",
      "[WARNING] No face detected in: dataset/dataset\\siddhangana\\photo_3_2025-03-13_14-23-34.jpg for siddhangana. Skipping.\n",
      "[INFO] Added 4 face images for siddhangana.\n",
      "[INFO] Processing images for: sparsh\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-005.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-010.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-015.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-020.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-025.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-030.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\ezgif-frame-035.jpg for sparsh. Skipping.\n",
      "[WARNING] Multiple faces (2) detected in: dataset/dataset\\sparsh\\photo_1_2025-03-06_14-37-02.jpg for sparsh. Using the first one.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\photo_4_2025-03-06_12-21-48.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\photo_6_2025-03-06_12-21-48.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\photo_8_2025-03-06_12-21-48.jpg for sparsh. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\sparsh\\photo_9_2025-03-06_12-21-48.jpg for sparsh. Skipping.\n",
      "[INFO] Added 19 face images for sparsh.\n",
      "[INFO] Processing images for: yash\n",
      "[WARNING] No face detected in: dataset/dataset\\yash\\photo_12_2025-03-06_12-21-48.jpg for yash. Skipping.\n",
      "[WARNING] No face detected in: dataset/dataset\\yash\\photo_15_2025-03-06_12-21-48.jpg for yash. Skipping.\n",
      "[WARNING] Multiple faces (2) detected in: dataset/dataset\\yash\\photo_17_2025-03-06_12-21-48.jpg for yash. Using the first one.\n",
      "[WARNING] Multiple faces (2) detected in: dataset/dataset\\yash\\photo_18_2025-03-06_12-21-48.jpg for yash. Using the first one.\n",
      "[WARNING] No face detected in: dataset/dataset\\yash\\photo_5_2025-03-06_14-34-55.jpg for yash. Skipping.\n",
      "[INFO] Added 23 face images for yash.\n",
      "\n",
      "[INFO] Total faces prepared for training: 199\n",
      "[INFO] Total unique persons (labels): 8\n",
      "[INFO] Label map: {0: 'amisha', 1: 'dhanoosh', 2: 'jose', 3: 'jui', 4: 'ritvi', 5: 'siddhangana', 6: 'sparsh', 7: 'yash'}\n",
      "[INFO] Training LBPH model...\n",
      "[INFO] LBPH model trained successfully.\n",
      "\n",
      "==============================\n",
      "[INFO] Starting model testing on the dataset...\n",
      "==============================\n",
      "[TESTING] Evaluating images for: amisha (Expected Label: 0)\n",
      "[TESTING] Finished amisha: Found faces in 17/21 images. Correctly recognized: 17/17\n",
      "[TESTING] Evaluating images for: dhanoosh (Expected Label: 1)\n",
      "[TESTING] Finished dhanoosh: Found faces in 64/101 images. Correctly recognized: 64/64\n",
      "[TESTING] Evaluating images for: jose (Expected Label: 2)\n",
      "[TESTING] Finished jose: Found faces in 23/29 images. Correctly recognized: 23/23\n",
      "[TESTING] Evaluating images for: jui (Expected Label: 3)\n",
      "[TESTING] Finished jui: Found faces in 19/20 images. Correctly recognized: 18/19\n",
      "[TESTING] Evaluating images for: ritvi (Expected Label: 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time # For FPS calculation (optional)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Main dataset folder containing subfolders named after people\n",
    "# Example structure:\n",
    "# /path/to/your/dataset/\n",
    "#  |- Person_A/\n",
    "#  |  |- img1.jpg\n",
    "#  |  |- img2.png\n",
    "#  |- Person_B/\n",
    "#  |  |- pic1.jpeg\n",
    "# ...\n",
    "dataset_path = \"dataset/dataset\"\n",
    "\n",
    "# Size to resize cropped faces to (consistency is key for LBPH)\n",
    "face_size = (100, 100)\n",
    "\n",
    "# LBPH Confidence threshold (lower values mean higher confidence)\n",
    "# Adjust this based on testing - might need values between 50 and 100\n",
    "confidence_threshold = 50 # Adjusted slightly for potentially better testing results\n",
    "\n",
    "# --- NEW: Testing Configuration ---\n",
    "# Set to True to test the model on the dataset after training\n",
    "activate_test = True\n",
    "\n",
    "# --- Global Variables ---\n",
    "# Load Dlib's pre-trained frontal face detector (HOG-based)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Create LBPH Face Recognizer\n",
    "# Note: You might need to install opencv-contrib-python\n",
    "# pip install opencv-contrib-python\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "# Dictionary to map numerical labels to names\n",
    "label_to_name = {}\n",
    "\n",
    "# --- Function to Prepare Training Data and Train LBPH ---\n",
    "def train_model(data_folder_path):\n",
    "    \"\"\"\n",
    "    Scans the dataset folder, detects faces, prepares training data,\n",
    "    and trains the LBPH recognizer.\n",
    "    Returns True if training was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Preparing training data from: {data_folder_path}\")\n",
    "    faces = []\n",
    "    labels = []\n",
    "    current_label_id = 0\n",
    "    global label_to_name # Allow modification of the global dictionary\n",
    "    label_to_name.clear() # Clear mapping for potential re-training runs\n",
    "\n",
    "    # Iterate through each person's folder in the dataset path\n",
    "    for person_name in os.listdir(data_folder_path):\n",
    "        person_folder_path = os.path.join(data_folder_path, person_name)\n",
    "\n",
    "        # Skip if it's not a directory\n",
    "        if not os.path.isdir(person_folder_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Processing images for: {person_name}\")\n",
    "        # Assign a numerical label to this person\n",
    "        if person_name not in label_to_name.values():\n",
    "            label_to_name[current_label_id] = person_name\n",
    "            person_label = current_label_id\n",
    "            current_label_id += 1\n",
    "        else:\n",
    "            # This case should ideally not happen if folder names are unique identifiers\n",
    "            person_label = [id for id, name in label_to_name.items() if name == person_name][0]\n",
    "\n",
    "\n",
    "        # Iterate through images in the person's folder\n",
    "        image_count = 0\n",
    "        for image_name in os.listdir(person_folder_path):\n",
    "            image_path = os.path.join(person_folder_path, image_name)\n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"[WARNING] Could not read image: {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Convert to grayscale (Dlib detector and LBPH work better with grayscale)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # Optional: Histogram Equalization can sometimes improve detection/recognition\n",
    "            # gray = cv2.equalizeHist(gray)\n",
    "\n",
    "            # Detect faces using Dlib detector\n",
    "            # Using upsampling=1 consistent with potential quality needs during training data prep\n",
    "            detected_faces = detector(gray, 1)\n",
    "\n",
    "            if len(detected_faces) == 0:\n",
    "                print(f\"[WARNING] No face detected in: {image_path} for {person_name}. Skipping.\")\n",
    "                continue\n",
    "            elif len(detected_faces) > 1:\n",
    "                 print(f\"[WARNING] Multiple faces ({len(detected_faces)}) detected in: {image_path} for {person_name}. Using the first one.\")\n",
    "\n",
    "            # Process only the first detected face for consistency\n",
    "            face_rect = detected_faces[0]\n",
    "            x, y, w, h = face_rect.left(), face_rect.top(), face_rect.width(), face_rect.height()\n",
    "\n",
    "            # Ensure coordinates are valid\n",
    "            x = max(0, x)\n",
    "            y = max(0, y)\n",
    "            if w <= 0 or h <= 0 or x + w > gray.shape[1] or y + h > gray.shape[0]:\n",
    "                print(f\"[WARNING] Invalid face bounds detected or calculated in: {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Crop the face region from the grayscale image\n",
    "            face_roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "            # Check if cropping resulted in an empty image (due to edge cases)\n",
    "            if face_roi.size == 0:\n",
    "                 print(f\"[WARNING] Empty face ROI after cropping in: {image_path}. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            # Resize the face ROI to the standard size\n",
    "            try:\n",
    "                resized_face = cv2.resize(face_roi, face_size, interpolation=cv2.INTER_AREA)\n",
    "            except cv2.error as e:\n",
    "                 print(f\"[WARNING] Error resizing face ROI from {image_path}: {e}. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            # Append the processed face and its corresponding label\n",
    "            faces.append(resized_face)\n",
    "            labels.append(person_label)\n",
    "            image_count += 1\n",
    "\n",
    "        print(f\"[INFO] Added {image_count} face images for {person_name}.\")\n",
    "\n",
    "\n",
    "    if not faces:\n",
    "        print(\"[ERROR] No faces found or processed for training. Check dataset path, image content, and warnings.\")\n",
    "        return False\n",
    "\n",
    "    if len(label_to_name) < 2:\n",
    "         print(\"[WARNING] Training requires images from at least two different people for the recognizer to be effective.\")\n",
    "         # Decide if you want to stop here or proceed with a potentially less useful model\n",
    "         # return False # Option to stop if less than 2 people\n",
    "\n",
    "    print(f\"\\n[INFO] Total faces prepared for training: {len(faces)}\")\n",
    "    print(f\"[INFO] Total unique persons (labels): {len(label_to_name)}\")\n",
    "    print(f\"[INFO] Label map: {label_to_name}\")\n",
    "    print(\"[INFO] Training LBPH model...\")\n",
    "\n",
    "    # Train the recognizer\n",
    "    # Ensure labels is a NumPy array of type int32\n",
    "    recognizer.train(faces, np.array(labels, dtype=np.int32))\n",
    "\n",
    "    print(\"[INFO] LBPH model trained successfully.\")\n",
    "    # Optional: Save the trained model and label mapping for later use\n",
    "    # recognizer.save(\"lbph_model.yml\")\n",
    "    # with open(\"label_map.txt\", \"w\") as f:\n",
    "    #  for label_id, name in label_to_name.items():\n",
    "    #     f.write(f\"{label_id}:{name}\\n\")\n",
    "    # print(\"[INFO] Model and label map saved.\")\n",
    "    return True\n",
    "\n",
    "# --- NEW: Function to Test Model on Dataset ---\n",
    "def test_model_on_dataset(data_folder_path):\n",
    "    \"\"\"\n",
    "    Tests the trained LBPH model on the images in the dataset folder\n",
    "    and calculates the success rate.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"[INFO] Starting model testing on the dataset...\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    global label_to_name, recognizer, detector, face_size, confidence_threshold\n",
    "\n",
    "    if not label_to_name:\n",
    "        print(\"[ERROR] Label map is empty. Cannot perform testing. Was the model trained?\")\n",
    "        return\n",
    "\n",
    "    # Create a reverse map for convenience (Name -> Label ID)\n",
    "    name_to_label = {name: label_id for label_id, name in label_to_name.items()}\n",
    "\n",
    "    total_tested_faces = 0\n",
    "    correct_predictions = 0\n",
    "    faces_detected_count = 0\n",
    "\n",
    "    # Iterate through each person's folder in the dataset path\n",
    "    for person_name in os.listdir(data_folder_path):\n",
    "        person_folder_path = os.path.join(data_folder_path, person_name)\n",
    "\n",
    "        if not os.path.isdir(person_folder_path):\n",
    "            continue\n",
    "\n",
    "        # Check if this person was part of the training set\n",
    "        if person_name not in name_to_label:\n",
    "            print(f\"[WARNING] Person '{person_name}' found in dataset but not in the trained label map. Skipping testing for this person.\")\n",
    "            continue\n",
    "\n",
    "        true_label_id = name_to_label[person_name]\n",
    "        print(f\"[TESTING] Evaluating images for: {person_name} (Expected Label: {true_label_id})\")\n",
    "\n",
    "        images_in_folder = 0\n",
    "        detections_in_folder = 0\n",
    "        correct_in_folder = 0\n",
    "\n",
    "        # Iterate through images in the person's folder\n",
    "        for image_name in os.listdir(person_folder_path):\n",
    "            image_path = os.path.join(person_folder_path, image_name)\n",
    "            images_in_folder += 1\n",
    "\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                # Warning already given during training, maybe skip here unless verbose testing needed\n",
    "                continue\n",
    "\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # gray = cv2.equalizeHist(gray) # Apply if used in training\n",
    "\n",
    "            # Use the same detection settings as training ideally\n",
    "            detected_faces = detector(gray, 1)\n",
    "\n",
    "            if len(detected_faces) == 0:\n",
    "                 # Can log this if needed: print(f\"[TESTING-WARN] No face detected in: {image_path}\")\n",
    "                 continue # Cannot test recognition if no face is found\n",
    "\n",
    "            faces_detected_count += 1\n",
    "            detections_in_folder += 1\n",
    "\n",
    "            # Use only the first detected face for consistency with training\n",
    "            face_rect = detected_faces[0]\n",
    "            x, y, w, h = face_rect.left(), face_rect.top(), face_rect.width(), face_rect.height()\n",
    "\n",
    "            x = max(0, x)\n",
    "            y = max(0, y)\n",
    "            if w <= 0 or h <= 0 or x + w > gray.shape[1] or y + h > gray.shape[0]:\n",
    "                continue # Skip invalid bounds\n",
    "\n",
    "            face_roi = gray[y:y+h, x:x+w]\n",
    "            if face_roi.size == 0:\n",
    "                 continue # Skip empty ROI\n",
    "\n",
    "            try:\n",
    "                resized_face = cv2.resize(face_roi, face_size, interpolation=cv2.INTER_AREA)\n",
    "            except cv2.error:\n",
    "                continue # Skip resize errors\n",
    "\n",
    "\n",
    "            # Perform prediction\n",
    "            predicted_label_id, confidence = recognizer.predict(resized_face)\n",
    "\n",
    "            total_tested_faces += 1 # Count this face as tested for recognition\n",
    "\n",
    "            # Evaluate prediction\n",
    "            prediction_is_correct = False\n",
    "            if confidence < confidence_threshold and predicted_label_id == true_label_id:\n",
    "                correct_predictions += 1\n",
    "                correct_in_folder += 1\n",
    "                prediction_is_correct = True\n",
    "\n",
    "            # Optional: Print detailed results per image (can be very verbose)\n",
    "            # predicted_name = label_to_name.get(predicted_label_id, \"Unknown Label\")\n",
    "            # print(f\"  - {image_name}: Predicted={predicted_name}({predicted_label_id}), Confidence={confidence:.2f}, Correct={prediction_is_correct}\")\n",
    "\n",
    "        print(f\"[TESTING] Finished {person_name}: Found faces in {detections_in_folder}/{images_in_folder} images. Correctly recognized: {correct_in_folder}/{detections_in_folder}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"[INFO] Dataset Testing Summary\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Total faces detected across all tested images: {faces_detected_count}\")\n",
    "    print(f\"Total detected faces subjected to recognition: {total_tested_faces}\")\n",
    "    print(f\"Total correct recognitions (within threshold): {correct_predictions}\")\n",
    "\n",
    "    if total_tested_faces > 0:\n",
    "        accuracy = (correct_predictions / total_tested_faces) * 100\n",
    "        print(f\"Recognition Success Rate: {accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(\"Recognition Success Rate: N/A (No faces were successfully processed for recognition testing)\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Function for Live Face Recognition ---\n",
    "def run_live_recognition():\n",
    "    \"\"\"\n",
    "    Opens the camera, detects faces, and performs recognition using the trained model.\n",
    "    \"\"\"\n",
    "    global label_to_name # Access the global mapping\n",
    "\n",
    "    print(\"[INFO] Starting video stream for live recognition...\")\n",
    "    # Use 0 for default camera, or change if you have multiple cameras\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    time.sleep(1.0) # Allow camera sensor to warm up\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"[ERROR] Cannot open camera. Exiting.\")\n",
    "        return\n",
    "\n",
    "    prev_time = 0\n",
    "\n",
    "    while True:\n",
    "        # Read frame from camera\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            print(\"[ERROR] Failed to grab frame. Exiting.\")\n",
    "            break\n",
    "\n",
    "        # For FPS calculation\n",
    "        current_time = time.time()\n",
    "        # Avoid division by zero on the first frame\n",
    "        time_diff = current_time - prev_time\n",
    "        fps = 1 / time_diff if time_diff > 0 else 0\n",
    "        prev_time = current_time\n",
    "\n",
    "        # Convert frame to grayscale for detection\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Optional: Equalize histogram\n",
    "        # gray_frame = cv2.equalizeHist(gray_frame)\n",
    "\n",
    "        # Detect faces in the current frame\n",
    "        # Using upsampling=0 for potentially faster live performance\n",
    "        detected_faces = detector(gray_frame, 0)\n",
    "\n",
    "        # Loop over detected faces\n",
    "        for face_rect in detected_faces:\n",
    "            x, y, w, h = face_rect.left(), face_rect.top(), face_rect.width(), face_rect.height()\n",
    "\n",
    "            # Ensure coordinates are valid and within frame boundaries\n",
    "            x = max(0, x)\n",
    "            y = max(0, y)\n",
    "            # Check width/height before calculating bottom-right corner\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "            # Adjust width/height if they extend beyond frame boundaries\n",
    "            w = min(w, frame.shape[1] - x)\n",
    "            h = min(h, frame.shape[0] - y)\n",
    "            # Re-check after adjustment\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Crop the face region *from the grayscale frame* for recognition\n",
    "            face_roi = gray_frame[y:y+h, x:x+w]\n",
    "\n",
    "             # Check if cropping resulted in an empty image (should be rare with checks above)\n",
    "            if face_roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Resize the cropped face to the standard size used for training\n",
    "            try:\n",
    "                resized_face = cv2.resize(face_roi, face_size, interpolation=cv2.INTER_AREA)\n",
    "            except cv2.error:\n",
    "                continue # Skip if resize fails\n",
    "\n",
    "\n",
    "            # Perform prediction using the trained LBPH recognizer\n",
    "            label_id, confidence = recognizer.predict(resized_face)\n",
    "\n",
    "            # Get the name associated with the predicted label ID\n",
    "            # Use \"Unknown\" if confidence is too high (poor match) or label unknown\n",
    "            name = \"Unknown\" # Default\n",
    "            print(confidence)\n",
    "            if label_id in label_to_name: # Check if label exists first\n",
    "                 if confidence > confidence_threshold:\n",
    "                      name = label_to_name[label_id]\n",
    "                      display_text = f\"{name} ({confidence:.2f})\"\n",
    "                 else:\n",
    "                      # Known person, but low confidence\n",
    "                      display_text = f\"Maybe {label_to_name[label_id]}? ({confidence:.2f})\"\n",
    "                      # Or just stick with \"Unknown\":\n",
    "                      # name = \"Unknown\"\n",
    "                      # display_text = f\"{name} ({confidence:.2f})\"\n",
    "            else:\n",
    "                 # Label ID not in our map (shouldn't happen if predict works correctly)\n",
    "                 display_text = f\"Unknown Label {label_id} ({confidence:.2f})\"\n",
    "\n",
    "\n",
    "            # Draw bounding box around the face on the *original color* frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Put the predicted name (and confidence) text above the bounding box\n",
    "            text_y = y - 10 if y - 10 > 10 else y + h + 20 # Adjust position if too close to top\n",
    "            cv2.putText(frame, display_text, (x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Display FPS on frame (optional)\n",
    "        cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Show the resulting frame\n",
    "        cv2.imshow(\"Live Face Recognition (Press 'q' to quit)\", frame)\n",
    "\n",
    "        # Check for exit key ('q')\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"[INFO] Stopping video stream and cleaning up...\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"[INFO] Application finished.\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.isdir(dataset_path):\n",
    "         print(f\"[ERROR] Dataset path not found or is not a directory: {dataset_path}\")\n",
    "         print(\"[INFO] Please set the 'dataset_path' variable correctly.\")\n",
    "    else:\n",
    "        # 1. Train the model\n",
    "        training_successful = train_model(dataset_path)\n",
    "\n",
    "        if training_successful:\n",
    "            # 2. Test the model (if activated)\n",
    "            if activate_test:\n",
    "                test_model_on_dataset(dataset_path)\n",
    "            else:\n",
    "                 print(\"[INFO] Skipping dataset testing as 'activate_test' is False.\")\n",
    "\n",
    "            # 3. Run live recognition (only if training was successful)\n",
    "            # Decide if you always want to run live, or maybe only if not testing, etc.\n",
    "            # Current logic: Run live recognition after training and optional testing.\n",
    "            run_live_recognition()\n",
    "        else:\n",
    "            print(\"[ERROR] Model training failed. Cannot proceed to testing or live recognition.\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
